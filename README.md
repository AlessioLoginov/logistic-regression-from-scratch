# Logistic Regression Optimization Methods

## Описание проекта

В этом проекте реализованы и сравнены три метода оптимизации для обучения модели логистической регрессии на данных о цветах ириса. Методы, рассмотренные в данном проекте, включают:
- **Gradient Descent** (Градиентный спуск)
- **RMSProp** (Метод скользящего среднего)
- **Nadam** (Ускоренный по Нестерову метод адаптивной оценки моментов).

## Набор данных

Использовался классический набор данных "Ирисы" из библиотеки `sklearn`. Из набора данных были оставлены только два класса: Iris Versicolor и Iris Virginica.

- **Форма X:** (100, 4) - 100 экземпляров с 4 признаками каждый
- **Форма y:** (100,) - метки классов (0 и 1)

## Методы оптимизации

### Gradient Descent
- **Точность:** 94.00%
- **Время работы:** 0.0198 секунд

### RMSProp
- **Точность:** 98.00%
- **Время работы:** 0.0262 секунд

### Nadam
- **Точность:** 94.00%
- **Время работы:** 0.0344 секунд

## Выводы

1. **RMSProp** показал наилучшую точность (98%) среди всех методов, что делает его наиболее эффективным для данного набора данных. Время работы немного больше, чем у Gradient Descent, но это оправдано улучшением точности.

2. **Gradient Descent** показал точность 94% и оказался самым быстрым методом (0.0198 секунд), что делает его подходящим для задач, где важна скорость, но точность не критична.

3. **Nadam** продемонстрировал ту же точность, что и Gradient Descent (94%), но потребовал больше времени на выполнение (0.0344 секунд). Это указывает на то, что для данного набора данных преимущества Nadam не так очевидны.

## Использование

Просто откройте файл `logistic_regression_optimization.ipynb` в Jupyter Notebook и запустите ячейки для выполнения всех шагов.

## Файлы проекта

- `logistic_regression_optimization.ipynb` - основной ноутбук с реализацией и сравнением методов оптимизации.
- `README.md` - описание проекта и инструкция по запуску.


